0. STORY
========

We need a nice problem-solving story !

Scope: datacenter networks, heavy hitters, host-based traffic engineering

	1. Heavy hitters are bad (in datacenter networks)
		- Heavy-Hitter Detection Entirely in the Data Plane: https://dl.acm.org/citation.cfm?id=3063772
		- Segment Routing in Massively Scalaboe Data Center: https://www.nanog.org/sites/default/files/Bhattacharya_Segment.pdf
		- other refs
	2. Show gap in current techniques against heavy hitters (i.e. no TE at end host)
		- refs
	3. Make the case for TE at the host (more powerful solution)
		- Talk about eBPF
		- SRN paper (big advantage here: applications can be totally agnostic to SRv6 or even IPv6)
		- Related: Carousel paper
		- other refs
	4. Extend technique to encompass reaction to TCP events
		- React to CC events and/or ECN marking
		- Benefits: can react to network congestion even if not detected by local HHF algo.
			- Mild congestion through ECN markings
			- Severe congestion or bad component on the path will induce CC events (e.g. RTO)
		- refs
	5. Implementation
		- Explain our journey: from CGROUP_SKB to LWT_XMIT to SCHED_ACT to SCHED_ACT + SOCK_OPS
		- CGROUP_SKB: read-only, filtering hook, per-cgroup
		- LWT_XMIT: read-write, probably requires an additional namespace, route and intf dependent
		- SCHED_ACT: most comprehensive write access, only intf dependent, will require a bridge or something to reduce MTU and to decouple accounting from policy enforcement
		- SOCK_OPS: per-cgroup, reaction to TCP events
	6. Evaluation
		- Performances considerations
			- Bridge traversal latency (vs veth traversal latency f.e.)
			- Cost of calling eBPF program
			- SRH encap cost (seg6_validate_srh() + seg6_do_srh_encap())
				- If seg6_validate_srh() is very costly, maybe maintain per-CPU list of already-validated SRHs ?
			- Adds a division in egress path due to gso_segs invalidated by SRH encap
	7. Future work
		- Show that this paper can be generalized to all TE scenarios, not just HH.

1. PAPER GOALS
==============

Goals:
- Detect heavy-hitters on the end-host (accounting and classification)
- Push SRH to steer heavy-hitters through different paths (steering)
- Leverage eBPF for all of the above (flexibility)
- Improve transport protocols (TCP, QUIC, MPTCP...) by using SRv6 (using SRv6
  to steer flow away from inefficient path)


Non-goals:
- Traffic shaping (simply use fq)
- Low latency

2. KERNEL SUPPORT
=================

2.1. Flow accounting
--------------------

One eBPF program on egress path, program type CGROUP_SKB, attach type CGROUP_INET_EGRESS.
Per-flow accounting in BPF map.

2.2. SRH encapsulation
----------------------

Internal function bpf_push_seg6_encap already implemented. Available through BPF helper
bpf_lwt_in_push_encap. We cannot use it as we are in the output/xmit path rather than
the input path. A helper bpf_lwt_xmit_push_encap exists but has no support for seg6
encap. Adding support for seg6 encap to it should be trivial, O(10) LOC changes.

SRH encapsulation program would be of type LWT_XMIT.

Caveats:
- Agnostic to cgroups
- Must be attached to a route
- Can be sharded per-namespace

Need to reconcile per-cgroup flow accounting with per-namespace policy enforcement.
Option: extend our CGROUP_SKB program to also mark the skb with a unique per-cgroup
socket mark. Encapsulation program will fetch SRH from eBPF map with key (5-tuple, mark).

2.3. Qdisc
----------

Traffic shaping is a non-goal but we want to have at least fairness between flows.
We can simply use fq for that. It will provide per-flow pacing. Shaping can be
optionally realized through the per-qdisc "maxrate" option or at the discretion
of the applcations through the SO_MAX_PACING_RATE socket option.

Interestingly, sch_fq provides a ce_threshold parameter, calling INET_ECN_set_ce()
for each skb whose actual xmit time exceeded its planned xmit time by more than ce_threshold
nanoseconds. Not sure if we can make use of that somehow.

3. USERSPACE CONTROLLER
=======================

A userspace controller would periodically iterate over the flow accounting table (eBPF map),
detect heavy hitters, and accordingly update the encapsulation eBPF map with appropriate SRHs.

For heavy hitters steering: two pools of SRH/BSIDs, one for HHs, one for non-HHs.
For flow steering re congestion event: pool of disjoints SRH/BSIDs ?

4. PERFORMANCES IMPLICATIONS
============================

The following items can impact the performances:
- Encapsulation (memcpy).
- For GSO skbs, gso_segs will be invalidated and recomputed in device layer. This adds a division in the fast path.

The following design choices can avoid performance losses:
- If we use LWT_XMIT instead of LWT_OUT, we do not need to do an additional route lookup. The encapsulated packet
  will be delivered to the original next-hop selected based on the inner header DA. This is perfectly fine on an
  end-host scenario.

5. CURRENT LIMITATIONS OF THE LINUX IMPLEMENTATION
==================================================

- The cgroup/skb program type doesn't allow to modify the skb->sk, so it can only be used for accounting
- The cgroup/sock program type allow modification on the sk, but can't be attached on the egress path

6. CURRENT IDEAS
================

- Encap in LW
- Find a way to do the setsockopt (difficult because the socket isn't locked)
